{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0094eb15-7d23-4f54-b6e4-75396da3cf6d",
   "metadata": {},
   "source": [
    "# Utils Script\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db0058-cee2-441c-8418-c5139690fd83",
   "metadata": {},
   "source": [
    "### This script consists of the different utility functions for the liteloan scorecard development \n",
    "### The Functions do a range of tasks from \n",
    "\n",
    "    - Computing PIR (Personal Indebtness ratio and FAI (Free Available Income) \n",
    "    - Compute other derived variables \n",
    "    - Plot graphs \n",
    "    - Create extensive analytical tables \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5c4fe3-4e0f-4541-91ec-2b20ebd91483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Util Libraries successfully imported\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import datetime as dt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import string \n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.metrics import fbeta_score\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from matplotlib import pyplot\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# from imblearn.under_sampling import TomekLinks\n",
    "# from imblearn.under_sampling import EditedNearestNeighbours\n",
    "# from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "# from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "# from imblearn.under_sampling import OneSidedSelection\n",
    "# from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "# from sklearn.linear_model import RidgeClassifier\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# from imblearn.combine import SMOTEENN\n",
    "# import statsmodels.api as sm\n",
    "# from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    \n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # from sklearn.linear_model import LogisticRegression\n",
    "# # from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# pd.set_option('max_rows', None)\n",
    "# pd.set_option('max_columns', None)\n",
    "# pd.set_option('max_colwidth', None)\n",
    "# pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "# plt.style.use('ggplot')\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "# color = sns.color_palette()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Util Libraries successfully imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c691a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31734a1-ddb4-4792-8176-60ee0a18fee5",
   "metadata": {},
   "source": [
    "## Compute Free Available Income (FAI) function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a194ccf6-cbae-4837-bc79-eec168900cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_income(df, db, cr, opening_balance): \n",
    "    \n",
    "    \"\"\"\n",
    "     %% parameters \n",
    "\n",
    "    df: [dataframe] - holding dataframe of income values \n",
    "\n",
    "    db: [str] - debit column \n",
    "\n",
    "    cr: [str]  - credit columns \n",
    "\n",
    "    opening_balance: [str] - opening balance column \n",
    "    \n",
    "    \n",
    "    ex: \n",
    "    \n",
    "    calc_income('total_credit', 'total_debit', 'opening_balance')\n",
    "    \"\"\"\n",
    "    \n",
    "    def cred_deb_list(df):\n",
    "        \n",
    "      #  \"\"\"\n",
    "      #  + This function takes in the in dataframe \n",
    "      #  + returns list of credit months, debit months\n",
    "      #  + returns as zipped or iterable list      \n",
    "      #  \"\"\"\" \n",
    "        try:\n",
    "            cred_month_list = [cred for cred in df.columns if cred.startswith(cr)]\n",
    "            debit_month_list = [deb for deb in df.columns if deb.startswith(db)]\n",
    "            month_list = list(zip(cred_month_list, debit_month_list))\n",
    "        except error as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            print('month lists have been extracted')\n",
    "        \n",
    "        return cred_month_list, debit_month_list, month_list \n",
    "    \n",
    "    credit_month_list, debit_month_list, month_list = cred_deb_list(df)\n",
    "    \n",
    "    \n",
    "    col_no = 1\n",
    "    cred_count = 0\n",
    "    deb_count = 0\n",
    "    opening_balance = df[opening_balance]\n",
    "    \n",
    "    for cred, deb in month_list:\n",
    "        \n",
    "        if col_no <= 7:\n",
    "            \n",
    "            df[\"FAI_\" + str(col_no)] = opening_balance + df[credit_month_list[cred_count]] - df[debit_month_list[deb_count]]\n",
    "            opening_balance =   df[\"FAI_\" + str(col_no)]\n",
    "            col_no +=1\n",
    "            cred_count += 1\n",
    "            deb_count += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            break \n",
    "            \n",
    "    return df \n",
    "                          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b355bb-184b-4b18-9dda-d5af81f0aa89",
   "metadata": {},
   "source": [
    "## Compute Personal Indebtnedness Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20dd88-ee90-450b-9b4f-ef80480127fc",
   "metadata": {},
   "source": [
    "+ PIR_1 = (𝑇𝑜𝑡𝑎𝑙 𝐼𝑛𝑠𝑡𝑎𝑙𝑙𝑚𝑒𝑛𝑡 𝐴𝑚𝑜𝑢𝑛𝑡 𝑃𝑎𝑖𝑑)/(Current FAI +𝑇𝑜𝑡𝑎𝑙 𝐼𝑛𝑠𝑡𝑎𝑙𝑙𝑚𝑒𝑛𝑡 𝐴𝑚𝑜𝑢𝑛𝑡 𝑃𝑎𝑖𝑑)  - Current working definition\n",
    "+ PIR_2 = (𝑇𝑜𝑡𝑎𝑙 𝐼𝑛𝑠𝑡𝑎𝑙𝑙𝑚𝑛𝑡 𝐴𝑚𝑜𝑢𝑛𝑡 𝑃𝑎𝑖𝑑+𝑅𝑒𝑛𝑀𝑜𝑛𝑒𝑦 𝐼𝑛𝑠𝑡𝑎𝑙𝑙𝑚𝑒𝑛𝑡)/(𝐶𝑙𝑜𝑠𝑖𝑛𝑔 𝐵𝑎𝑙𝑎𝑛𝑐𝑒+𝑇𝑜𝑡𝑎𝑙 𝐼𝑛𝑠𝑡𝑎𝑙𝑙𝑚𝑒𝑛𝑡 𝐴𝑚𝑜𝑢𝑛𝑡 𝑃𝑎𝑖𝑑)\n",
    "+ PIR_3 = (𝑇𝑜𝑡𝑎𝑙 𝑃𝑟𝑖𝑛𝑐𝑖𝑝𝑎𝑙 𝑅𝑒𝑐𝑒𝑖𝑣𝑒𝑑)/(𝐶𝑙𝑜𝑠𝑖𝑛𝑔 𝐵𝑎𝑙𝑎𝑛𝑐𝑒−𝑇𝑜𝑡𝑎𝑙 𝑃𝑟𝑖𝑛𝑐𝑖𝑝𝑎𝑙 𝑅𝑒𝑐𝑒𝑖𝑣𝑒𝑑)\n",
    "+ PIR_4 = (𝑇𝑜𝑡𝑎𝑙 𝑃𝑟𝑖𝑛𝑐𝑖𝑝𝑎𝑙 𝑅𝑒𝑐𝑒𝑖𝑣𝑒𝑑+𝑃𝑟𝑖𝑛𝑐𝑖𝑝𝑎𝑙𝑅𝑒𝑛𝑀𝑜𝑛𝑒𝑦)/(𝐶𝑙𝑜𝑠𝑖𝑛𝑔 𝐵𝑎𝑙𝑎𝑛𝑐𝑒−𝑇𝑜𝑡𝑎𝑙 𝑃𝑟𝑖𝑛𝑐𝑖𝑝𝑎𝑙 𝑅𝑒𝑐𝑒𝑖𝑣𝑒𝑑(𝑁𝑜𝑛𝑅𝑒𝑛𝑚𝑜𝑛𝑒𝑦))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c939047-148c-4073-ab63-f2c95f8c2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_std(df):\n",
    "    FAI_cols = [col for col in df.columns if col.startswith(\"FAI\")]\n",
    "    df['FAI_Avg'] = df[FAI_cols].mean(axis=1)\n",
    "    df['FAI_std'] = df[FAI_cols].std(axis=1)\n",
    "    df['FAI_min_debit'] = df[debit_month_list].min(axis=1)\n",
    "    df['FAI_max_debit'] = df[debit_month_list].max(axis=1)\n",
    "    df['FAI_min_credit'] = df[credit_month_list].min(axis=1)\n",
    "    df['FAI_max_credit'] = df[credit_month_list].max(axis=1)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aedde1d-989e-450d-af87-b150beb006c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PIR(df, loan_repayment_col, loan_Id, closing_FAI, no_months):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df: [dataframe] -  holding dataframe of loan data \n",
    "    \n",
    "    loan_repayment_col: [str] - column repayment name \n",
    "    \n",
    "    loan_id: [str] - column ID \n",
    "    \n",
    "    closing_FAI: [str] - closing Free Available Income \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #filter out repayment columns \n",
    "    \n",
    "    repayment_cols = [col for col in df.columns if col.startswith(loan_repayment_col)]\n",
    "    filtered_cols = repayment_cols + [loan_Id]\n",
    "    repayment_df = df.filter(filtered_cols)\n",
    "    \n",
    "    #remove duplicate columns \n",
    "    dupl_cols = [col for col in df.columns if col in repayment_df.columns]\n",
    "    dupl_cols.remove(loan_Id)\n",
    "    \n",
    "    to_merge_df = df.drop(dupl_cols, axis=1)\n",
    "    \n",
    "    #merge on Loan Id to comp_df\n",
    "    \n",
    "    repayment_df = to_merge_df.merge(repayment_df, on=loan_Id, how='outer')\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    #compute on PIR_1 \n",
    "    \n",
    "    total_installment_paid = repayment_df[repayment_cols].sum(axis=1)\n",
    "    closing_balance = repayment_df[closing_FAI]\n",
    "    repayment_df['total_installment_paid'] = total_installment_paid \n",
    "    average_installment_paid = repayment_df['total_installment_paid'] / no_months\n",
    "    repayment_df['average_installment_paid'] = average_installment_paid \n",
    "    repayment_df['PIR_1'] = average_installment_paid / (closing_balance + average_installment_paid)\n",
    "    \n",
    "    \n",
    "    return repayment_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bce2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to calculate WOE and IV for numerical features\n",
    "def woe_num(data, feature, gbflag):\n",
    "    dt = pd.crosstab(index=data[feature], columns=data[gbflag])\n",
    "    dt['Freq'] = dt.sum(axis=1)\n",
    "    dt['Percentage'] = round((dt['Freq']/dt['Freq'].sum() *100),1)\n",
    "    dt['% Good'] = round((dt['Good']/dt['Good'].sum() *100), 1)\n",
    "    dt['% Bad'] = round((dt['Bad']/dt['Bad'].sum() *100), 1)\n",
    "    dt['Bad Rate'] = round((dt['Bad']/dt['Freq']) *100, 1)\n",
    "    dt['GoodBaddOdds'] = round(dt['Good']/dt['Bad'], 2)\n",
    "    dt['WOE'] = np.log(dt['% Good']/dt['% Bad'])\n",
    "    dt['IV'] = (dt['% Good']- dt['% Bad']) * dt['WOE']\n",
    "#     dt['IV'] = ((dt['% Good']- dt['% Bad']) * dt['WOE']).sum()\n",
    "   # dt = dt.sort_values(['WOE'])\n",
    "    \n",
    "    return dt\n",
    "# Function to calculate WOE and IV for categorical variables\n",
    "# Function to calculate WOE and IV for categorical variables\n",
    "def woe_cat(data, feature, gbflag):\n",
    "    dt = pd.crosstab(index=data[feature], columns=data[gbflag])\n",
    "    dt['Freq'] = dt.sum(axis=1)\n",
    "    dt['Proptn'] = dt['Freq']/dt['Freq'].sum()\n",
    "    dt['% Good'] = dt['Good']/dt['Good'].sum() \n",
    "    dt['% Bad'] = dt['Bad']/dt['Bad'].sum()\n",
    "    dt['Bad Rate'] = dt['Bad']/dt['Freq'] \n",
    "    dt['GoodBaddOdds'] = round(dt['Good']/dt['Bad'], 2)\n",
    "    dt['WOE'] = np.log(dt['% Bad']/dt['% Good'])  # Ratio of Event to Non-Event\n",
    "    dt['class IV'] = (dt['% Bad']-dt['% Good']) * dt['WOE']  # IV for each class/categpry\n",
    "    dt['Variable IV'] = ((dt['% Bad']-dt['% Good']) * dt['WOE']).sum() #IV for the variable\n",
    "    dt = dt.sort_values(['WOE'])\n",
    "    \n",
    "    return dt\n",
    "# A function to get all variables WOE and IV\n",
    "def woe_iv(df, variablie_list, GBFlag):   \n",
    "    output = {}\n",
    "    for variable in variablie_list:\n",
    "        try:\n",
    "            var = woe_cat(df, variable, GBFlag)\n",
    "            \n",
    "            for i in range(len(var.index.values)):\n",
    "                output.setdefault('Variables',[]).append(variable)\n",
    "                output.setdefault('Categories',[]).append(var.index.values[i])\n",
    "                output.setdefault('WOE',[]).append(var['WOE'].values[i])\n",
    "                output.setdefault('IV',[]).append(var['IV'].values[i])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame(output)\n",
    "# woe_iv(data_universe_fai, data_universe_fai_cat.columns, 'GBFlag')\n",
    "# A function that automates and return CA report\n",
    "def Export_CA_Report(df, variablie_list, GBFlag, fileName='CA_Report'):\n",
    "    \n",
    "    # This inner function estimates WoE and IV which represent CA for all the provided variables\n",
    "    def CA_Report(df, variablie_list, GBFlag):\n",
    "        # Create a dictionary to track CA iteratively\n",
    "        output = {}\n",
    "        for variable in variablie_list:\n",
    "            # use the function for estimating WoE for categorical variables to \n",
    "            var = woe_cat(df, variable, GBFlag)\n",
    "            # append to dictionary\n",
    "            output[variable]=var        \n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "   # Instatiate above function to CA report for all provided variables\n",
    "    ca_result = CA_Report(df, variablie_list, GBFlag)\n",
    "    # save CA report to Excel\n",
    "    with pd.ExcelWriter(fileName +'.xlsx', engine=\"openpyxl\") as writer:\n",
    "        for variable_name, data in ca_result.items():\n",
    "            data.to_excel(writer, sheet_name=variable_name)\n",
    "    \n",
    "    \n",
    "def fill_binned_nan_values(df, variablie_list, GBFlag):\n",
    "    for variable in variablie_list:\n",
    "        if df[variable].isna().sum() > 0:\n",
    "            # Run CA(WoE & IV) on variable\n",
    "            run_ca = woe_num(df, variable, GBFlag)\n",
    "            # get index value(bin class) of category with least woe\n",
    "            bin_class = run_ca[run_ca['WOE']==max(run_ca['WOE'])].index.values[0]\n",
    "            df[variable] = df[variable].fillna(bin_class)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8febc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517647e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_FAI(df, total_credit_cols, total_debit_cols, opening_bal):\n",
    "    # get all total credit columns\n",
    "    cred_month_list = [cred for cred in df.columns if cred.startswith(total_credit_cols)]\n",
    "    # get all total debit columns\n",
    "    debit_month_list = [deb for deb in df.columns if deb.startswith(total_debit_cols)]\n",
    "    # get initial opening balance\n",
    "    opening_balance = df[opening_bal]\n",
    "    # compute FAI_1 from opening balance\n",
    "    df['FAI_1'] = opening_balance + df[cred_month_list[0]] - df[debit_month_list[0]]\n",
    "    # compute subsequent FAIs from previous FAI values\n",
    "    for i in range(1, len(cred_month_list)):\n",
    "        df[\"FAI_\" + str(i+1)] = df['FAI_'+str(i)] + df[cred_month_list[i]] - df[debit_month_list[i]]\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# A function to quickly search for variables in large dataframe\n",
    "def search_variable(df, variable):\n",
    "    res =  [col for col in df.columns if variable.lower() in col.lower()]\n",
    "    if res:\n",
    "        return res\n",
    "    return 'Not Found in DataFrame'\n",
    "\n",
    "def convert_from_lower_to_upper(df1, list):\n",
    "    \n",
    "    '''\n",
    "    function to map old ss variable name to new one for PSI\n",
    "    '''\n",
    "    \n",
    "    res= [col for col in df1.columns]\n",
    "    a = len(res)\n",
    "    \n",
    "    while a>0:\n",
    "        for var in res:\n",
    "            for var1 in list:\n",
    "                if var == var1.lower():\n",
    "                    df1.rename(columns = {var: var1}, inplace = True)\n",
    "                    \n",
    "        a-=1\n",
    "        \n",
    "    return df1\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f073dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_zero_values_table(df):\n",
    "  \n",
    "    \"\"\"\n",
    "    function for data audit, returns the count and percentage of missing and o values in each column\n",
    "     %% parameters \n",
    "\n",
    "    df: [dataframe] \n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    zero_val = (df == 0.00).astype(int).sum(axis=0)\n",
    "        \n",
    "    zero_val_percent = (df == 0.00).astype(int).sum(axis=0) / len(df)\n",
    "        \n",
    "    mis_val = df.isnull().sum()\n",
    "    \n",
    "    mis_val_percent = round(df.isnull().sum() / len(df),1)\n",
    "    \n",
    "    mz_table = pd.concat([zero_val,zero_val_percent, mis_val, mis_val_percent], axis=1)\n",
    "    \n",
    "    mz_table = mz_table.rename(\n",
    "                                columns = {0 : 'No of 0s', 1 : '% of 0s', 2: 'No of Missing Values', 3 : '% of Missing Values'})\n",
    "    \n",
    "    mz_table['Rows'] = len(df)\n",
    "    mz_table['No of Unique'] = df.nunique()\n",
    "    \n",
    "    mz_table['Data Type'] = df.dtypes\n",
    "    \n",
    "    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n",
    "        \"There are \" + str(mz_table.shape[0]) +\n",
    "            \" columns that have missing values.\")\n",
    "    \n",
    "#         mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)\n",
    "    return mz_table\n",
    "\n",
    "\n",
    "def audit(data):\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for col in data.columns:\n",
    "        No_unique = data[col].nunique()\n",
    "        Missing = data[col].isna().sum()\n",
    "        perc_missing = Missing/data.shape[0]\n",
    "        Zeros = data[data[col] == 0].shape[0]\n",
    "        perc_zeros = Zeros/data.shape[0]\n",
    "        row_count = data[col].shape[0]\n",
    "        \n",
    "        result.setdefault('Variable', []).append(col)\n",
    "        result.setdefault('No of Unique', []).append(No_unique)\n",
    "        result.setdefault('No of Missing', []).append(Missing)\n",
    "        result.setdefault('% Missing', []).append(perc_missing)\n",
    "        result.setdefault('No of Zeros', []).append(Zeros)\n",
    "        result.setdefault('% Zeros', []).append(perc_zeros)\n",
    "        result.setdefault('No of Rows', []).append(row_count)\n",
    "        \n",
    "    res = pd.DataFrame(result)\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec509949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "\n",
    "def display_side_by_side(*args):\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += df.to_html()\n",
    "    display_html(html_str.replace(\n",
    "        'table', 'table style=\"display:inline\"'), raw=True)\n",
    "    \n",
    "    \n",
    "def sample_first_prows(data, perc=0.8):\n",
    "    '''function to get specified percentage of first rows in the dataframe\n",
    "    '''\n",
    "    return data.head(int(len(data)*(perc)))\n",
    "\n",
    "# train = sample_first_prows(data)\n",
    "# test = data.iloc[max(train.index):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73100766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_binned_nan_values(df, variablie_list, GBFlag):\n",
    "    for variable in variablie_list:\n",
    "        ## Coarse NaN group into the worst woe group if not more than 10% of total data shape\n",
    "      \n",
    "            # Run CA(WoE & IV) on variable using the woe_num function\n",
    "            run_ca = woe_num(df, variable, GBFlag)\n",
    "            # get index value(bin class) of category with worst woe category\n",
    "            bin_class = run_ca[run_ca['WOE']==max(run_ca['WOE'])].index.values[0]\n",
    "            df[variable] = df[variable].fillna(bin_class)\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eb4a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_woe(df_with_selected_variables):\n",
    "    '''\n",
    "    This function replaces each category in the selected columns with\n",
    "    their woe\n",
    "    \n",
    "    df_with_selected_variables :: Dataframe containing grouped/binned selected\n",
    "    variables\n",
    "    \n",
    "    grpd_variables_woe_iv :: Dataframe containing WoE and IV of grouped/binned\n",
    "    variables\n",
    "    \n",
    "    Note:: Make sure woe_iv function has returned it's result before running\n",
    "    this function. result in this case - grpd_variables_woe_iv\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for variable in df_with_selected_variables.columns:\n",
    "        for category in df_with_selected_variables[variable].unique():\n",
    "            \n",
    "            try:\n",
    "                ### Get WoE from the dataframe running the WoE_IV function\n",
    "                woe =variables_woe_iv[\n",
    "                    (variables_woe_iv['Variables']==variable) & \n",
    "                    (variables_woe_iv['Categories']==category)\n",
    "                ]['WOE'].values[0]\n",
    "\n",
    "                ### Replace each category with their respective woe\n",
    "                df_with_selected_variables[variable] = df_with_selected_variables[variable].replace(category, woe)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    return df_with_selected_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd185f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A function to get all variables WOE and IV\n",
    "# def woe_iv(df, variablie_list, GBFlag):   \n",
    "#     output = {}\n",
    "#     for variable in variablie_list:\n",
    "#         try:\n",
    "#             var = woe_cat(df, variable, GBFlag)\n",
    "            \n",
    "#             for i in range(len(var.index.values)):\n",
    "#                 output.setdefault('Variables',[]).append(variable)\n",
    "#                 output.setdefault('Categories',[]).append(var.index.values[i])\n",
    "#                 output.setdefault('WOE',[]).append(var['WOE'].values[i])\n",
    "#                 output.setdefault('Variable IV',[]).append(var['Variable IV'].values[i])\n",
    "#         except Exception as e:\n",
    "#             pass\n",
    "\n",
    "#     return pd.DataFrame(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A function to get all variables WOE and IV\n",
    "def woe_iv(df, variablie_list, GBFlag):   \n",
    "    output = {}\n",
    "    for variable in variablie_list:\n",
    "        try:\n",
    "            var =  (pd.crosstab(df[variable],df[GBFlag],normalize='columns').assign(woe=lambda dfx: np.log(dfx['Bad'] / dfx['Good'])).assign(iv=lambda dfx: np.sum(dfx['woe']*(dfx['Bad']-dfx['Good']))))\n",
    "            \n",
    "            for i in range(len(var.index.values)):\n",
    "                output.setdefault('Variables',[]).append(variable)\n",
    "                output.setdefault('Categories',[]).append(var.index.values[i])\n",
    "                output.setdefault('WOE',[]).append(var['woe'].values[i])\n",
    "                output.setdefault('Variable IV',[]).append(var['iv'].values[i])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame(output)\n",
    "\n",
    "def iv(df, variablie_list, GBFlag):   \n",
    "    output = {}\n",
    "    for variable in variablie_list:\n",
    "        try:\n",
    "            var = woe_cat(df, variable, GBFlag)\n",
    "            \n",
    "            for i in range(len(var.index.values)):\n",
    "                output.setdefault('Variables',[]).append(variable)\n",
    "                output.setdefault('Variable IV',[]).append(var['Variable IV'].values[i])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491a8ef",
   "metadata": {},
   "source": [
    "##### undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c80f8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## modeling\n",
    "\n",
    "## undersampling\n",
    "\n",
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "\treturn fbeta_score(y_true, y_pred, beta=2)\n",
    " \n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# define the model evaluation metric\n",
    "\tmetric = make_scorer(f2_measure)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    " \n",
    "# define undersampling models to test\n",
    "def get_models():\n",
    "\tmodels, names = list(), list()\n",
    "\t# TL\n",
    "\tmodels.append(TomekLinks())\n",
    "\tnames.append('TL')\n",
    "\t# ENN\n",
    "\tmodels.append(EditedNearestNeighbours())\n",
    "\tnames.append('ENN')\n",
    "\t# RENN\n",
    "\tmodels.append(RepeatedEditedNearestNeighbours())\n",
    "\tnames.append('RENN')\n",
    "\t# OSS\n",
    "\tmodels.append(OneSidedSelection())\n",
    "\tnames.append('OSS')\n",
    "\t# NCR\n",
    "\tmodels.append(NeighbourhoodCleaningRule())\n",
    "\tnames.append('NCR')\n",
    "  #InstanceHardnessThreshold\n",
    "\n",
    "\tmodels.append(InstanceHardnessThreshold())\n",
    "\tnames.append('IHT')\n",
    "\treturn models, names\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07cb111c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fcae88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function helps in computing Gini from AUC\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/66679372/how-does-the-predict-function-of-statsmodels-interact-with-roc-auc-score-of-scik\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "def compute_gini(y_test, y_train, test_preds, train_preds):\n",
    "    # instantiate roc auc score from sci-kit learn metrics and compute AUC\n",
    "\n",
    "    Test_AUC = roc_auc_score(y_test, test_preds)\n",
    "    Train_AUC = roc_auc_score(y_train, train_preds)\n",
    "    # Compute Gini from AUC\n",
    "    Test_Gini = (2*Test_AUC) - 1\n",
    "    Train_Gini = (2*Train_AUC) - 1\n",
    "    return Test_Gini, Train_Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(y_test, y_train, test_preds, train_preds):\n",
    "    # instantiate roc auc score from sci-kit learn metrics and compute AUC\n",
    "    train_preds = train_preds.to_list()\n",
    "    train_preds = np.asarray(train_preds)\n",
    "    test_preds = test_preds.to_list()\n",
    "    test_preds = np.asarray(test_preds)\n",
    "    Test_precision, Test_recall, thresh = precision_recall_curve(y_test, test_preds)\n",
    "    Train_precision, Train_recall, train_thresh = precision_recall_curve(y_train, train_preds)\n",
    "\n",
    "    return Test_recall, Train_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(y_test, y_train, test_preds, train_preds):\n",
    "    # instantiate roc auc score from sci-kit learn metrics and compute AUC\n",
    "    train_preds = train_preds.to_list()\n",
    "    train_preds = np.asarray(train_preds)\n",
    "    test_preds = test_preds.to_list()\n",
    "    test_preds = np.asarray(test_preds)\n",
    "    Test_precision, Test_recall, thresh = precision_recall_curve(y_test, test_preds)\n",
    "    Train_precision, Train_recall, train_thresh = precision_recall_curve(y_train, train_preds)\n",
    "\n",
    "    return Test_precision, Train_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07fc7034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.5       , 1.        , 1.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# y_true = np.array([0, 0, 1, 1])\n",
    "# y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "# precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "# precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203a350",
   "metadata": {},
   "source": [
    "#### Gini test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def univariate_gini_computation(df, features, target):\n",
    "\n",
    "    \n",
    "#     # create an empty dictionary\n",
    "#     gini_ = {}\n",
    "    \n",
    "#     # iteratively get the features from a list of features\n",
    "#     for feature in features:\n",
    "#         # extrac the feature and target from the entire dataframe\n",
    "#         data = df[[feature, target]]\n",
    "#         # encode the target variable; Good-0, Bad-1\n",
    "#         data[target] = np.where(data[target]=='Bad', 1, 0)\n",
    "#         # get dummies for the feature (independent variable)\n",
    "#         data = pd.concat([data, pd.get_dummies(data[feature], prefix=feature, prefix_sep='-')],axis=1)\n",
    "#         # drop the feature since we have its dummies\n",
    "#         data = data.drop(feature, axis=1)\n",
    "        \n",
    "#         # Split data for training\n",
    "#         X = data.drop(target, axis=1).values\n",
    "#         y = data[target].values\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "        \n",
    "#         # instantiate logistic regression algo\n",
    "#         lr = LogisticRegression()\n",
    "#         # fit data to the algorithm\n",
    "#         lr.fit(X_train, y_train)\n",
    "#         # predict probability of default (PD) from test data\n",
    "#         y_pred = lr.predict_proba(X_test)\n",
    "#         # extract the PD\n",
    "#         y_pred = y_pred[:][:, 1]\n",
    "        \n",
    "#         # Estimate AUC\n",
    "#         AUC = roc_auc_score(y_test, y_pred)\n",
    "#         # Compute Gini from AUC\n",
    "#         Gini = 2*AUC - 1\n",
    "        \n",
    "#         # Append both feature and Gini to the above dictionary\n",
    "#         gini_.setdefault('Feature',[]).append(feature)\n",
    "#         gini_.setdefault('Gini',[]).append(Gini)\n",
    "        \n",
    "#         # All result to be returned in a dataframe\n",
    "#         result = pd.DataFrame(gini_)\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65234ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):\n",
    "    x = np.array(df_WoE.index).astype(str)\n",
    "    y = df_WoE['WOE']\n",
    "    plt.figure(figsize = (26, 8))\n",
    "    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')\n",
    "    #plt.xlabel(x)\n",
    "    plt.ylabel('Weight of Evidence')\n",
    "    plt.title(str(''))\n",
    "    plt.xticks(rotation = rotation_of_x_axis_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a294196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_equal(data):\n",
    "  result = {}\n",
    "  result['Variable1'] = []\n",
    "  result['Variable2'] = []\n",
    "  var = data.columns.to_list()\n",
    "  for i in range(len(var)-1):\n",
    "    for col in var:\n",
    "      if (data[var[i]].equals(data[col])) & (var[i]!=col):\n",
    "        if col in result['Variable2']:\n",
    "          pass\n",
    "        else:\n",
    "         result['Variable1'].append(col)\n",
    "         result['Variable2'].append(var[i])\n",
    "         \n",
    "         \n",
    "  result = pd.DataFrame(data = result)\n",
    "  return result"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "974f694f54a02f493e12eb6b1cb55f7f9224502eeb4003e7f82b2e996b0d3812"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('clv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
